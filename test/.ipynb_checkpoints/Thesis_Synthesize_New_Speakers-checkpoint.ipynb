{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES AND SETUP ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import time\n",
    "import glob\n",
    "import argparse\n",
    "import librosa\n",
    "import pyworld\n",
    "import pysptk\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import fnmatch\n",
    "import subprocess\n",
    "import soundfile as sf\n",
    "import IPython\n",
    "import csv\n",
    "\n",
    "sys.path.append(\"/home/fa578s/thesis-mss-vector-synthesis-TTS-Portuguese-Corpus/\") \n",
    "# add libraries into environment\n",
    "VOCODER_PATH = \"/home/fa578s/hifi-gan/\"\n",
    "sys.path.append(VOCODER_PATH)\n",
    "\n",
    "#Set this if TTS is not installed globally or problems occur with TTS\n",
    "#TTS_PATH = \"/home/user/thesis-mss-vector-synthesis\"\n",
    "#sys.path.append(TTS_PATH) \n",
    "\n",
    "#INSERT ROOT DIRECTORY PATH UP TO THE PRIMARY GITREPO\n",
    "HOME_DIRECTORY = \"/home/fa578s/\"\n",
    "\n",
    "### SET OUTPUT AUDIO PATH ###\n",
    "OUT_PATH = '/home/fa578s/Desktop/outputs/tristin/'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from IPython.display import Audio\n",
    "from tqdm import tqdm\n",
    "from scipy.io.wavfile import read as read_wav\n",
    "from scipy.io.wavfile import write\n",
    "from env import AttrDict\n",
    "#from attrdict import AttrDict\n",
    "from models import Generator\n",
    "from TTS.tts.utils.generic_utils import setup_model\n",
    "from TTS.tts.utils.synthesis import synthesis\n",
    "from TTS.tts.utils.text.symbols import make_symbols, phonemes, symbols\n",
    "from scipy import spatial\n",
    "from TTS.utils.io import load_config\n",
    "from TTS.vocoder.utils.generic_utils import setup_generator\n",
    "\n",
    "try:\n",
    "  from TTS.utils.audio import AudioProcessor\n",
    "except:\n",
    "  from TTS.utils.audio import AudioProcessor\n",
    "\n",
    "#Check is cuda is avaliable.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "\n",
    "#These configs tell the speaker encoders where to find the specific wav files.\n",
    "INPUT_DATASET_CONFIG = \"/home/fa578s/thesis-mss-vector-synthesis/audios/input/input_dataset_config.json\"\n",
    "MSS_DATASET_CONFIG = \"/home/fa578s/thesis-mss-vector-synthesis/audios/output/mss_dataset_config.json\"\n",
    "\n",
    "\n",
    "#Original WAV embeddings\n",
    "#mss_pre_embedding_file = HOME_DIRECTORY + \"thesis-mss-vector-synthesis/audios/input/audio/speakers.json\"\n",
    "mss_pre_embedding_file = HOME_DIRECTORY + \"/thesis-mss-vector-synthesis/audios/input/audio/speakers.json\"\n",
    "\n",
    "#Synthesized WAV embeddings\n",
    "#mss_after_embedding_file = HOME_DIRECTORY + \"thesis-mss-vector-synthesis/audios/output/audio/speakers.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL CHECKPOINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HIFI-GAN CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HiFi-GAN Vocoder ###\n",
    "hifi_gan_checkpoint = '/home/fa578s/miniconda3/envs/My_project/checkpoints/hifi_gan/generator.pt'\n",
    "hifi_gan_config_file = os.path.join(os.path.split(hifi_gan_checkpoint)[0], 'config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS MODEL CHECKPOINTS\n",
    "Uncomment one model path from each model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MSS-Vectors Based TTS Model ###\n",
    "#30k step speaker encoder output vector size 512 \n",
    "MSS_TTS_MODEL_PATH = '/home/fa578s/miniconda3/envs/My_project/checkpoints/mss_vector/tts/experiment_2/30kmss_vsize_512_checkpoint_280000.pth.tar'\n",
    "\n",
    "#50k step speaker encoder model\n",
    "#MSS_TTS_MODEL_PATH = '../checkpoints/mss_vector/tts/experiment_1/50kmss_checkpoint_280000.pth.tar'\n",
    "\n",
    "#30k step speaker encoder model\n",
    "#MSS_TTS_MODEL_PATH = '../checkpoints/mss_vector/tts/experiment_1/30kmss_checkpoint_280000.pth.tar'\n",
    "\n",
    "MSS_TTS_CONFIG_PATH = os.path.join(os.path.split(MSS_TTS_MODEL_PATH)[0], 'config.json')\n",
    "\n",
    "\n",
    "# ### S-Vector Based TTS Model ###\n",
    "# S_VECTOR_TTS_MODEL_PATH = '../checkpoints/s_vector/tts/30ks_checkpoint_280000.pth.tar'\n",
    "# S_VECTOR_TTS_CONFIG_PATH = os.path.join(os.path.split(S_VECTOR_TTS_MODEL_PATH)[0], 'config.json')\n",
    "    \n",
    "    \n",
    "### GE2E Based TTS Model ###\n",
    "#50k step speaker encoder model\n",
    "#GE2E_TTS_MODEL_PATH = '../checkpoints/ge2e/tts/30kge2e_checkpoint_280000.pth.tar'\n",
    "\n",
    "# #30k step speaker encoder model\n",
    "# GE2E_TTS_MODEL_PATH = '../checkpoints/ge2e/tts/30kge2e_checkpoint_280000.pth.tar'\n",
    "\n",
    "# GE2E_TTS_CONFIG_PATH = os.path.join(os.path.split(GE2E_TTS_MODEL_PATH)[0], 'config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPEAKER ENCODER CHECKPOINTS\n",
    "Uncomment one model path from each model. Speaker encoder should correspond to model used in TTS Checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MSS SPEAKER ENCODER ###\n",
    "#30k step speaker encoder output vector size 512\n",
    "MSS_ENCODER_PATH = \"/home/fa578s/miniconda3/envs/My_project/checkpoints/mss_vector/se/experiment_2/mss_vsize_512_checkpoint_30000.pth.tar\"\n",
    "\n",
    "#50k step \n",
    "#MSS_ENCODER_PATH = \"../checkpoints/mss_vector/se/experiment_1/mss_checkpoint_50000.pth.tar\"\n",
    "\n",
    "#30k step\n",
    "#MSS_ENCODER_PATH = \"../checkpoints/mss_vector/se/experiment_1/mss_checkpoint_30000.pth.tar\"\n",
    "\n",
    "MSS_CONFIG = os.path.join(os.path.split(MSS_ENCODER_PATH)[0], 'config.json')\n",
    "\n",
    "\n",
    "### S-VECTOR SPEAKER ENCODER ###\n",
    "# S_VECTOR_ENCODER_PATH = \"../checkpoints/s_vector/se/sv_checkpoint_30000.pth.tar\"\n",
    "# S_VECTOR_CONFIG = os.path.join(os.path.split(S_VECTOR_ENCODER_PATH)[0], 'config.json')\n",
    "\n",
    "\n",
    "# ### GE2E SPEAKER ENCODER ###\n",
    "# #50k step \n",
    "# #GE2E_ENCODER_PATH = \"../checkpoints/ge2e/se/ge2e_checkpoint_50000.pth.tar\"\n",
    "\n",
    "# #30k step\n",
    "# GE2E_ENCODER_PATH = \"../checkpoints/ge2e/se/ge2e_checkpoint_30000.pth.tar\"\n",
    "\n",
    "# GE2E_CONFIG = os.path.join(os.path.split(GE2E_ENCODER_PATH)[0], 'config.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPEAKER ENCODER UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_files(input_dir):\n",
    "    #Checks if files in a given directory need to be resampled to make the 24kHz sample rate of the TTS models\n",
    "    need_resampled = []\n",
    "    audio_files = glob.glob(os.path.join(input_dir, \"*.wav\"))\n",
    "    #print(audio_files)\n",
    "    print(f\"Found {len(audio_files)} total files...\")\n",
    "    for file in audio_files:\n",
    "        sampling_rate, data=read_wav(file)\n",
    "        if(sampling_rate != 24000):\n",
    "            need_resampled.append(file)\n",
    "    print(f\"Found {len(need_resampled)} files that need resampled...\")       \n",
    "    with tqdm(total=len(need_resampled)) as pbar:\n",
    "        for file in need_resampled:\n",
    "            pbar.update()\n",
    "            y, sr = librosa.load(file, 24000)\n",
    "            sf.write(file, y, sr)\n",
    "\n",
    "def compute_embeddings(model_path, config, dataset, input_or_output, model_name):\n",
    "    #Computes embeddings with help of premade program that comes with CoquiTTS\n",
    "    command = \"source /home/tristin/miniconda3/bin/activate tts_training && python3 ../thesis-mss-vector-training/TTS/bin/compute_embeddings.py \\\n",
    "    {} \\\n",
    "    {} \\\n",
    "    {} \\\n",
    "    ./audios/{}/{} && source /home/tristin/miniconda3/bin/deactivate\".format(model_path, config, dataset, input_or_output, model_name)\n",
    "    subprocess.run(command, shell=True, executable='/bin/bash')\n",
    "    \n",
    "def find_speaker_wavs(speaker, src_dir):\n",
    "    #Helper function for load_speaker embeddings\n",
    "    wavs = []\n",
    "    for filename in enumerate(glob.glob(src_dir+\"*.wav\")):\n",
    "        if (fnmatch.fnmatch(filename[1], '*'+speaker+'*')):\n",
    "            wavs.append(filename[1].split('/')[-1])\n",
    "    return wavs\n",
    "\n",
    "def load_speaker_embeddings(speakers, embedding_file, src_dir, avg=False):\n",
    "    #Load speaker embeddings from speaker.json file\n",
    "    speakers_info = {}\n",
    "    f = open(embedding_file)\n",
    "    embeddings = json.load(f)\n",
    "    for speaker_id in speakers:\n",
    "        #print(\"\\nSPEAKER ID: {}\".format(speaker_id))\n",
    "        wav_files = find_speaker_wavs(speaker_id, src_dir)\n",
    "        #print(\"source dir is: {}\".format(src_dir))\n",
    "        #print(\"################################################\")\n",
    "        #print(\"# found wav files: {}\".format(len(wav_files)))\n",
    "        if len(wav_files) > 1:\n",
    "            if avg:\n",
    "                tmp_file = \"\"\n",
    "                tmp_index = 0\n",
    "                emb = np.empty([len(wav_files), 256])\n",
    "                for i, file_name in enumerate(wav_files):\n",
    "                    vector = embeddings[file_name][\"embedding\"]\n",
    "                    emb[i] = vector\n",
    "                    tmp_file = file_name\n",
    "                    tmp_index = i\n",
    "                emb = np.mean(emb, axis=0)\n",
    "                speakers_info[tmp_file] = speaker_id + \"_avg_\" + str(i+1), emb\n",
    "                print(speaker_id + \"_avg_\" + str(i+1))\n",
    "            else:\n",
    "                for file_name in wav_files:\n",
    "                    speakers_info[file_name] = speaker_id, embeddings[file_name][\"embedding\"]\n",
    "        else:\n",
    "            speakers_info[wav_files[0]] = speaker_id, embeddings[wav_files[0]][\"embedding\"]\n",
    "        \n",
    "    f.close()\n",
    "    return speakers_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS MODEL UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tts(model, text, CONFIG, use_cuda, ap, use_gl, speaker_fileid, speaker_embedding=None, gst_style=None, silent=False):\n",
    "    t_1 = time.time()\n",
    "    t_2_s = time.time()\n",
    "    waveform, _, _, mel_postnet_spec, _, _ = synthesis(model, text, CONFIG, use_cuda, ap, speaker_fileid, gst_style, False, CONFIG.enable_eos_bos_chars, use_gl, speaker_embedding=speaker_embedding)\n",
    "    t_2_e = time.time()\n",
    "    if not silent:\n",
    "        print(\"CONFIG.model: \", CONFIG.model)\n",
    "    if CONFIG.model == \"Tacotron\" and not use_gl:\n",
    "        mel_postnet_spec = ap.out_linear_to_mel(mel_postnet_spec.T).T\n",
    "\n",
    "    waveform_voc = hifi_predict(torch.FloatTensor(mel_postnet_spec.T).unsqueeze(0).cuda(), silent)\n",
    "    waveform_voc = waveform_voc.flatten()\n",
    "    rtf2 = (t_2_e - t_2_s)/ (len(waveform_voc) / ap.sample_rate)\n",
    "    rtf = (time.time() - t_1) / (len(waveform_voc) / ap.sample_rate)\n",
    "    tps = (time.time() - t_1) / len(waveform_voc)\n",
    "    if not silent:\n",
    "        print(\" > Run-time: {}\".format(time.time() - t_1))\n",
    "        print(\" > Real-time factor without vocoder: {}\".format(rtf2))\n",
    "        print(\" > Real-time factor: {}\".format(rtf))\n",
    "        print(\" > Time per step: {}\".format(tps))\n",
    "    return waveform, waveform_voc\n",
    "\n",
    "speaker_embedding = None\n",
    "use_griffin_lim = False\n",
    "speaker_embedding_dim = 512\n",
    "def load_tts_model(CONFIG_PATH, MODEL_PATH, speaker_embedding_dim):\n",
    "    # load the config\n",
    "    C = load_config(CONFIG_PATH)\n",
    "\n",
    "    # load the audio processor\n",
    "    ap = AudioProcessor(**C.audio)\n",
    "\n",
    "    # if the vocabulary was passed, replace the default\n",
    "    if 'characters' in C.keys():\n",
    "        symbols, phonemes = make_symbols(**C.characters)\n",
    "    \n",
    "    # load the tts model\n",
    "    num_chars = len(phonemes) if C.use_phonemes else len(symbols)\n",
    "    model = setup_model(num_chars, 20, C, speaker_embedding_dim)\n",
    "    cp = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(cp['model'])\n",
    "    model.eval()\n",
    "\n",
    "    if USE_CUDA:\n",
    "        model = model.cuda()\n",
    "\n",
    "    model.length_scale = 1 # set speed of the speech. \n",
    "    model.noise_scale = 0.0 # set speech variation\n",
    "    return model, ap, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD TTS MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:24000\n",
      " | > num_mels:80\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.98\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > stats_path:None\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Using model: glow_tts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fa578s/thesis-mss-vector-synthesis-TTS-Portuguese-Corpus/TTS/utils/audio.py:92: FutureWarning: Pass sr=24000, n_fft=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.filters.mel(\n",
      "/home/fa578s/thesis-mss-vector-synthesis-TTS-Portuguese-Corpus/TTS/tts/layers/glow_tts/glow.py:160: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2426.)\n",
      "  w_init = torch.qr(\n",
      "/home/fa578s/.local/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "#if changing mss vector dimension change the vector size here as the last arguement.\n",
    "mss_model, mss_ap, mss_c = load_tts_model(MSS_TTS_CONFIG_PATH, MSS_TTS_MODEL_PATH, 512)\n",
    "# s_vector_model, s_vector_ap, s_vector_c = load_tts_model(S_VECTOR_TTS_CONFIG_PATH, S_VECTOR_TTS_MODEL_PATH, 512)\n",
    "# ge2e_model, ge2e_ap, ge2e_c = load_tts_model(GE2E_TTS_CONFIG_PATH, GE2E_TTS_MODEL_PATH, 256) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIFI-GAN UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = None\n",
    "device = None\n",
    "\n",
    "MAX_WAV_VALUE = 32767.5\n",
    "\n",
    "def load_checkpoint(filepath, device):\n",
    "    assert os.path.isfile(filepath)\n",
    "    print(\"Loading '{}'\".format(filepath))\n",
    "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
    "    print(\"Complete.\")\n",
    "    return checkpoint_dict\n",
    "\n",
    "\n",
    "def scan_checkpoint(cp_dir, prefix):\n",
    "    pattern = os.path.join(cp_dir, prefix + '*')\n",
    "    cp_list = glob.glob(pattern)\n",
    "    if len(cp_list) == 0:\n",
    "        return ''\n",
    "    return sorted(cp_list)[-1]\n",
    "\n",
    "def hifi_predict(spectrogram, silent=False):\n",
    "    start = time.time()\n",
    "    y_g_hat = generator(spectrogram)\n",
    "    audio = y_g_hat.squeeze()\n",
    "    audio = audio * MAX_WAV_VALUE\n",
    "    audio = audio.detach().cpu().numpy().astype('int16')\n",
    "    if not silent:\n",
    "        print('HiFi-GAN Time', time.time()-start)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD HIFI-GAN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading '/home/fa578s/miniconda3/envs/My_project/checkpoints/hifi_gan/generator.pt'\n",
      "Complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(hifi_gan_config_file) as f:\n",
    "    data = f.read()\n",
    "    json_config = json.loads(data)\n",
    "    h = AttrDict(json_config)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "generator = Generator(h).to(device)\n",
    "\n",
    "state_dict_g = load_checkpoint(hifi_gan_checkpoint, device)\n",
    "generator.load_state_dict(state_dict_g['generator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCD & COSINE SIMILARITY UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 24000\n",
    "FRAME_PERIOD = 5.0\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    #Calculates the cosine similarity\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "def load_wav(wav_file, sr):\n",
    "    \"\"\"\n",
    "    Load a wav file with librosa.\n",
    "    :param wav_file: path to wav file\n",
    "    :param sr: sampling rate\n",
    "    :return: audio time series numpy array\n",
    "    \"\"\"\n",
    "    wav, _ = librosa.load(wav_file, sr=sr, mono=True)\n",
    "\n",
    "    return wav\n",
    "\n",
    "def log_spec_dB_dist(x, y):\n",
    "    log_spec_dB_const = 10.0 / math.log(10.0) * math.sqrt(2.0)\n",
    "    diff = x - y\n",
    "    \n",
    "    return log_spec_dB_const * math.sqrt(np.inner(diff, diff))\n",
    "\n",
    "def wav2mcep_numpy(wavfile, target_directory, alpha=0.65, fft_size=512, mcep_size=34):\n",
    "    # make relevant directories\n",
    "    if not os.path.exists(target_directory):\n",
    "        os.makedirs(target_directory)\n",
    "\n",
    "    loaded_wav = load_wav(wavfile, sr=SAMPLING_RATE)\n",
    "\n",
    "    # Use WORLD vocoder to spectral envelope\n",
    "    _, sp, _ = pyworld.wav2world(loaded_wav.astype(np.double), fs=SAMPLING_RATE,\n",
    "                                   frame_period=FRAME_PERIOD, fft_size=fft_size)\n",
    "\n",
    "    # Extract MCEP features\n",
    "    mgc = pysptk.sptk.mcep(sp, order=mcep_size, alpha=alpha, maxiter=0,\n",
    "                           etype=1, eps=1.0E-8, min_det=0.0, itype=3)\n",
    "\n",
    "    fname = os.path.basename(wavfile).split('.')[0]\n",
    "    \n",
    "    #Save mceps to numpy file\n",
    "    np.save(os.path.join(target_directory, fname + '.npy'),\n",
    "            mgc,\n",
    "            allow_pickle=False)\n",
    "\n",
    "def average_mcd(ref_mcep_files, synth_mcep_files, cost_function):\n",
    "    \"\"\"\n",
    "    Calculate the average MCD.\n",
    "    :param ref_mcep_files: list of strings, paths to MCEP target reference files\n",
    "    :param synth_mcep_files: list of strings, paths to MCEP converted synthesised files\n",
    "    :param cost_function: distance metric used\n",
    "    :returns: average MCD, total frames processed\n",
    "    \"\"\"\n",
    "    min_cost_tot = 0.0\n",
    "    frames_tot = 0\n",
    "    \n",
    "    for ref in ref_mcep_files:\n",
    "        for synth in synth_mcep_files:\n",
    "            # get the reference and synthesized speaker names and ids\n",
    "            ref_fsplit, synth_fsplit = os.path.basename(ref).split('_'), os.path.basename(synth).split('_')\n",
    "            ref_spk, ref_id = ref_fsplit[0], ref_fsplit[-1]\n",
    "\n",
    "            if (len(synth_fsplit) > 4):\n",
    "                synth_spk, synth_id = synth_fsplit[2], synth_fsplit[-1]\n",
    "            else:\n",
    "                synth_spk, synth_id = synth_fsplit[1], synth_fsplit[-1]\n",
    "                \n",
    "            # if the speaker name is the same and sample id is the same, calculate MCD\n",
    "            if ref_spk == synth_spk and ref_id == synth_id:\n",
    "                # load MCEP vectors\n",
    "                ref_vec = np.load(ref)\n",
    "                ref_frame_no = len(ref_vec)\n",
    "                synth_vec = np.load(synth)\n",
    "\n",
    "                # dynamic time warping using librosa\n",
    "                min_cost, _ = librosa.sequence.dtw(ref_vec[:, 1:].T, synth_vec[:, 1:].T, \n",
    "                                                   metric=cost_function)\n",
    "                \n",
    "                min_cost_tot += np.mean(min_cost)\n",
    "                frames_tot += ref_frame_no\n",
    "\n",
    "    mean_mcd = min_cost_tot / frames_tot\n",
    "    \n",
    "    return mean_mcd, frames_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesizing Test Voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 total files...\n",
      "Found 0 files that need resampled...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#Example Prompt\n",
    "#hello world. I am a synthetic voice produced by a zero-shot text-to-speech synthesis system.\n",
    "\n",
    "AVG = False\n",
    "FIRST_COMPUTE = True\n",
    "\n",
    "#The speaker names for ids\n",
    "unseen_speakers = [\"christina_arctic\", \"tristin_arctic\", \"r1\"]\n",
    "\n",
    "# create output path if it doesnt exist\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "\n",
    "#Resample files to 24kHz if needed\n",
    "resample_files(\"/home/fa578s/thesis-mss-vector-synthesis/audios/input/audio/\")\n",
    "\n",
    "#initialize file list and mcd variables\n",
    "reference_file_list = []\n",
    "\n",
    "mss_more_similar = 0\n",
    "other_mss_more_similar = 0\n",
    "i = 0\n",
    "mss_mcd_lower = 0\n",
    "\n",
    "\n",
    "        # s_vector_out_path = os.path.join(OUT_PATH, \"s_vector/\" + (\"avg_\" if AVG else \"\") + \"s_vector_\" + wav_file)\n",
    "        # s_vector_ap.save_wav(s_vector_wav, s_vector_out_path) \n",
    "        \n",
    "        # ge2e_out_path = os.path.join(OUT_PATH, \"ge2e/\" + (\"avg_\" if AVG else \"\") + \"ge2e_\" + wav_file)\n",
    "        # ge2e_ap.save_wav(ge2e_wav, ge2e_out_path) \n",
    "    \n",
    "    # #Compute embeddings of synthesized samples\n",
    "    # compute_embeddings(MSS_ENCODER_PATH, MSS_CONFIG, MSS_DATASET_CONFIG, \"output\", \"mss\")\n",
    "    # # compute_embeddings(S_VECTOR_ENCODER_PATH, S_VECTOR_CONFIG, S_VECTOR_DATASET_CONFIG, \"output\",  \"s_vector\")\n",
    "    # # compute_embeddings(GE2E_ENCODER_PATH, GE2E_CONFIG, GE2E_DATASET_CONFIG, \"output\", \"ge2e\")\n",
    "    \n",
    "    # #load the synthesized embeddings\n",
    "    # mss_unseen_speaker_after_embeddings = load_speaker_embeddings(unseen_speakers, mss_after_embedding_file, src_dir=\"./audios/output/audio/mss/\", avg=AVG)\n",
    "    # s_vector_unseen_speaker_after_embeddings = load_speaker_embeddings(unseen_speakers, s_vector_after_embedding_file, src_dir=\"./audios/output/audio/s_vector/\", avg=AVG)\n",
    "    # ge2e_unseen_speaker_after_embeddings = load_speaker_embeddings(unseen_speakers, ge2e_after_embedding_file, src_dir=\"./audios/output/audio/ge2e/\", avg=AVG)\n",
    "    \n",
    "    # for file_name in reference_file_list:\n",
    "    #     i = i + 1\n",
    "    #     #speaker_name = mss_unseen_speaker_preembeddings[file_name.split('/')[-1]][0]\n",
    "    #     speaker_name = file_name.split('/')[-1].split('.')[0]#.split('_')[0]\n",
    "    #     wav_file =file_name.split('/')[-1]\n",
    "        \n",
    "    #     #Aquire proper embedding from each model the for given reference file\n",
    "    #     s_vector_synthesized_speaker_embedding = s_vector_unseen_speaker_after_embeddings[\"s_vector_\" + wav_file][1]\n",
    "    #     mss_synthesized_speaker_embedding = mss_unseen_speaker_after_embeddings[\"mss_\" + wav_file][1]\n",
    "    #     ge2e_synthesized_speaker_embedding = ge2e_unseen_speaker_after_embeddings[\"ge2e_\" + wav_file][1]\n",
    "        \n",
    "    #     #Calculate cosine similarity\n",
    "    #     mss_similarity = cos_sim(mss_speaker_embedding, mss_synthesized_speaker_embedding)\n",
    "    #     s_vector_similarity = cos_sim(s_vector_speaker_embedding, s_vector_synthesized_speaker_embedding)\n",
    "    #     ge2e_similarity = cos_sim(ge2e_speaker_embedding, ge2e_synthesized_speaker_embedding)\n",
    "    #     if mss_similarity > s_vector_similarity:\n",
    "    #         mss_more_similar = mss_more_similar + 1\n",
    "        \n",
    "    #     print(\"\\n########################################################################\")\n",
    "    #     print(\"Synthesize sentence with Speaker: \", speaker_name)\n",
    "    #     _, mss_wav = tts(mss_model, TEXT, mss_c, USE_CUDA, mss_ap, use_griffin_lim, None, speaker_embedding=mss_speaker_embedding, silent=True)\n",
    "    #     _, s_vector_wav = tts(s_vector_model, TEXT, s_vector_c, USE_CUDA, s_vector_ap, use_griffin_lim, None, speaker_embedding=s_vector_speaker_embedding, silent=True)\n",
    "    #     _, ge2e_wav = tts(ge2e_model, TEXT, ge2e_c, USE_CUDA, ge2e_ap, use_griffin_lim, None, speaker_embedding=ge2e_speaker_embedding, silent=True)\n",
    "        \n",
    "    #     print(\"\\nSynthesized: MSS Vector\\nCosine Similarity: {}\".format(mss_similarity))\n",
    "    #     IPython.display.display(Audio(mss_wav, rate=mss_ap.sample_rate))\n",
    "        \n",
    "    #     print(\"Synthesized: S Vector\\nCosine Similarity: {}\".format(s_vector_similarity))\n",
    "    #     IPython.display.display(Audio(s_vector_wav, rate=s_vector_ap.sample_rate))\n",
    "        \n",
    "    #     print(\"Synthesized: GE2E\\nCosine Similarity: {}\".format(ge2e_similarity))\n",
    "    #     IPython.display.display(Audio(ge2e_wav, rate=ge2e_ap.sample_rate))\n",
    "        \n",
    "    #     print(\"Original\")\n",
    "    #     IPython.display.display(Audio(file_name, rate=s_vector_ap.sample_rate))\n",
    "\n",
    "    #     #Set path to wave file for mcd calculation\n",
    "    #     mss_out_path = os.path.join(OUT_PATH, \"mss/\" + \"mss_\" + wav_file)\n",
    "    #     s_vector_out_path = os.path.join(OUT_PATH, \"s_vector/\" + \"s_vector_\" + wav_file)\n",
    "    #     ge2e_out_path = os.path.join(OUT_PATH, \"ge2e/\" + \"ge2e_\" + wav_file)\n",
    "        \n",
    "    #     #MCD calculation variables\n",
    "    #     alpha = 0.65  # commonly used at 22050 Hz\n",
    "    #     fft_size = 512\n",
    "    #     mcep_size = 34\n",
    "        \n",
    "    #     #Directories of where mcep files are stored\n",
    "    #     mss_mcep_dir = \"./audios/mceps_numpy/mss/\"\n",
    "    #     s_vector_mcep_dir = \"./audios/mceps_numpy/s_vector/\"\n",
    "    #     ge2e_mcep_dir = \"./audios/mceps_numpy/ge2e/\"\n",
    "    #     ref_mcep_dir = \"./audios/mceps_numpy/ref/\"\n",
    "        \n",
    "    #     #Generate mcep files\n",
    "    #     wav2mcep_numpy(file_name, ref_mcep_dir, fft_size=fft_size, mcep_size=mcep_size)\n",
    "    #     wav2mcep_numpy(mss_out_path, mss_mcep_dir, fft_size=fft_size, mcep_size=mcep_size)\n",
    "    #     wav2mcep_numpy(s_vector_out_path, s_vector_mcep_dir, fft_size=fft_size, mcep_size=mcep_size)\n",
    "    #     wav2mcep_numpy(ge2e_out_path, ge2e_mcep_dir, fft_size=fft_size, mcep_size=mcep_size)\n",
    "        \n",
    "    #     #Find mcep files\n",
    "    #     ref_mceps = glob.glob(\"./audios/mceps_numpy/ref/*\")\n",
    "    #     mss_mceps = glob.glob(\"./audios/mceps_numpy/mss/*\")\n",
    "    #     s_vector_mceps = glob.glob(\"./audios/mceps_numpy/s_vector/*\")\n",
    "    #     ge2e_mceps = glob.glob(\"./audios/mceps_numpy/ge2e/*\")\n",
    "        \n",
    "    #     cost_function = log_spec_dB_dist\n",
    "        \n",
    "    #     #Calculate MCD\n",
    "    #     mss_mcd, mss_tot_frames_used = average_mcd(ref_mceps, mss_mceps, cost_function)\n",
    "    #     s_vector_mcd, s_vector_tot_frames_used = average_mcd(ref_mceps, s_vector_mceps, cost_function)\n",
    "    #     ge2e_mcd, ge2e_tot_frames_used = average_mcd(ref_mceps, ge2e_mceps, cost_function)\n",
    "\n",
    "    #     print(f'MSS MCD = {mss_mcd} dB, calculated over a total of {mss_tot_frames_used} frames')\n",
    "    #     print(f'Common MCD = {s_vector_mcd} dB, calculated over a total of {s_vector_tot_frames_used} frames')\n",
    "    #     print(f'GE2E MCD = {ge2e_mcd} dB, calculated over a total of {ge2e_tot_frames_used} frames')\n",
    "        \n",
    "    #     #remove mcep files\n",
    "    #     os.system(\"rm ./audios/mceps_numpy/*/*.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentence:  hello this is a test audio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text: hello this is a test audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: /home/tristin/miniconda3/bin/activate: No such file or directory\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary +: 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m _, mss_wav \u001b[38;5;241m=\u001b[39m tts(mss_model, TEXT, mss_c, USE_CUDA, mss_ap, use_griffin_lim, \u001b[38;5;28;01mNone\u001b[39;00m, speaker_embedding\u001b[38;5;241m=\u001b[39mmss_speaker_embedding, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# _, s_vector_wav = tts(s_vector_model, TEXT, s_vector_c, USE_CUDA, s_vector_ap, use_griffin_lim, None, speaker_embedding=s_vector_speaker_embedding, silent=True)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# _, ge2e_wav = tts(ge2e_model, TEXT, ge2e_c, USE_CUDA, ge2e_ap, use_griffin_lim, None, speaker_embedding=ge2e_speaker_embedding, silent=True)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#Save files to appropriate directory\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m mss_out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUT_PATH, \u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mAVG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmss_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m wav_file)\n\u001b[1;32m     39\u001b[0m mss_ap\u001b[38;5;241m.\u001b[39msave_wav(mss_wav, mss_out_path) \n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary +: 'str'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    TEXT =  input(\"Enter sentence: \")\n",
    "    if TEXT == 'q':\n",
    "        break\n",
    "    print(\" > Text: {}\".format(TEXT))\n",
    "    \n",
    "    #Compute initial speaker embeddings\n",
    "    compute_embeddings(MSS_ENCODER_PATH, MSS_CONFIG, INPUT_DATASET_CONFIG, \"input\", \"mss\")\n",
    "    # compute_embeddings(S_VECTOR_ENCODER_PATH, S_VECTOR_CONFIG, INPUT_DATASET_CONFIG, \"input\",  \"s_vector\")\n",
    "    # compute_embeddings(GE2E_ENCODER_PATH, GE2E_CONFIG, INPUT_DATASET_CONFIG, \"input\", \"ge2e\")\n",
    "    \n",
    "    #load initial speaker embeddings\n",
    "    mss_unseen_speaker_preembeddings = load_speaker_embeddings(unseen_speakers, mss_pre_embedding_file, src_dir=\"/home/fa578s/thesis-mss-vector-synthesis/audios/input/audio/\", avg=AVG)\n",
    "    # s_vector_unseen_speaker_preembeddings = load_speaker_embeddings(unseen_speakers, s_vector_pre_embedding_file, src_dir=\"./audios/input/audio/\", avg=AVG)\n",
    "    # ge2e_unseen_speaker_preembeddings = load_speaker_embeddings(unseen_speakers, ge2e_pre_embedding_file, src_dir=\"./audios/input/audio/\", avg=AVG)\n",
    "\n",
    "    #populate file list\n",
    "    for file_name in mss_unseen_speaker_preembeddings.keys():\n",
    "        reference_file_list.append(\"/home/fa578s/thesis-mss-vector-synthesis/audios/input/audio/\" + file_name)    \n",
    "        \n",
    "    for file_name in reference_file_list:\n",
    "        i = i + 1\n",
    "        speaker_name = mss_unseen_speaker_preembeddings[file_name.split('/')[-1]][0]\n",
    "        #speaker_name = file_name.split('/')[-1].split('.')[0]#.split('_')[0]\n",
    "        wav_file =file_name.split('/')[-1]\n",
    "     \n",
    "        #get the proper presynthesis speaker embedding\n",
    "        mss_speaker_embedding = mss_unseen_speaker_preembeddings[wav_file][1]\n",
    "        # s_vector_speaker_embedding = s_vector_unseen_speaker_preembeddings[wav_file][1]\n",
    "        # ge2e_speaker_embedding = ge2e_unseen_speaker_preembeddings[wav_file][1]\n",
    "        \n",
    "        #Synthesize wave files\n",
    "        _, mss_wav = tts(mss_model, TEXT, mss_c, USE_CUDA, mss_ap, use_griffin_lim, None, speaker_embedding=mss_speaker_embedding, silent=True)\n",
    "        # _, s_vector_wav = tts(s_vector_model, TEXT, s_vector_c, USE_CUDA, s_vector_ap, use_griffin_lim, None, speaker_embedding=s_vector_speaker_embedding, silent=True)\n",
    "        # _, ge2e_wav = tts(ge2e_model, TEXT, ge2e_c, USE_CUDA, ge2e_ap, use_griffin_lim, None, speaker_embedding=ge2e_speaker_embedding, silent=True)\n",
    "\n",
    "        #Save files to appropriate directory\n",
    "        mss_out_path = os.path.join(OUT_PATH, (\"avg_\" if AVG else \"\") + \"mss_\" + wav_file)\n",
    "        mss_ap.save_wav(mss_wav, mss_out_path) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a68e56f7aa15b5a646e7a36241e62fce19efa96bacf5f148e0cafff2e168dfe2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
